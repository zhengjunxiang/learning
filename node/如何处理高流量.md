[原文链接](https://juejin.cn/post/7125341011731185694)

# 多级缓存

页面缓存 + redis缓存

# 改进渲染模式

将CSR改成 ---> 将获取数据和html简单拼装

```
/**
* html拼接方法
* @param pageData，问卷题目配置数据
* @param resouceInfo，前端资源版本控制，由服务端控制做长缓存更新使用
*/
function render(pageData，resouceInfo){
 const html = `<!DOCTYPE html>
 <html lang="en">
 <head>
  <title>${pageData.title || '问卷'}</title>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
  <meta http-equiv="X-UA-COMPATIBLE" content="IE=Edge,chrome=1">
  <meta name="nightmode" content="disable">
  <link rel="shortcut icon" type="image/x-icon" href="//wenjuan-static-cn.heytapimage.com/wj-prod/faviconIcon.ico" id="favicon">
  <!--问卷页面样式文件-->
  <link href="${resouceInfo['main-css']}"  rel="stylesheet">
  <!--babel垫片-->
  <script content="babel-polyfill" nomodule  src="${resouceInfo['babel-polyfill']}"></script>
  <!--vue全家桶-->
  <script content="vue-all" src="${resouceInfo['vue-all']}"></script>
 </head>
 <body ${bodyStyle}>
  <div id="app"></div>
    <script content="page-data">
      // pageData 直出到页面并挂在在window，方便vue使用
      window.asyncData = ${JSON.stringify(pageData)};
    </script>
    <!--问卷页面入口js-->
    <script content="main-js"  src="${resouceInfo['main-js']}"></script>
  </body>
  </html>`
  
  return html
}
```

# 使用kafka处理并发接口
[消息队列原文](https://juejin.cn/post/6844903904203784199)

消息队列使用场景

* 解耦  https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2019/8/1/16c4c063caa0743c~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.awebp
* 异步 https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2019/8/1/16c4c063cb60a88d~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.awebp
* 削峰 https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2019/8/1/16c4c063cbb2140e~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.awebp

消息队列消费方式

点对点、发布订阅

## Kafka是一个流处理平台

* 发布&订阅流式数据，类似于消息队列或企业消息传递系统。
* 在高容错方式下保存流式数据
* 当数据流产生时实时进行处理

Kafka 主要应用在两个类应用中

构建可在系统或应用程序之前构建可靠获取数据的实时数据流管道
构建一个转换或响应数据流的实时数据流应用程序

* Producer： 生产者，发送信息的服务端
* Consumer：消费者，订阅消息的客户端
* Broker：消息中间件处理节点，一个Kafka节点就是一个broker，一个或者多个Broker可以组成一个Kafka集群
* Topic: 主题，可以理解成队列
* ConsumerGroup：消费者组，一个 ConsumerGoup 里面包括多个 Consumer，每个 ConsumerGoup 里面只有一个 Consumer 可以消费一个 Topic。基于这个特性，每个 ConsumerGoup 里面只存一个 Consumer 可以实现广播；所有 Consumer 都存在于同一个 ConsumerGoup 内则可以实现单播。
* Partition：基于 Kafka 的拓展性，有可能一个很大的 Topic 会存在于不同的 Broker 里面。这时一个 Topic 里面就会存在多个 Partition，Partition 是一个有序的队列，Partition 上每个消息会有一个顺序的 id —— Offset。但是，值得注意的是，Kafka 会保证 Partition 的顺序性，而没有保证 Topic 的顺序性。
* Offset：Kafka 的存储文件都是offset顺序存储的，以 offset.kafka 来命名。例如第一个就是 0000.kafka, 第 n 个文件就是 n-1.kafka
* Zookeerper：管理多个 Kafka 节点，具有管理集群配置的功能


## Kafka Nodejs 实现
### 消费方式：点对点
单个消费者的实现，应用场景是只有一个消费者节点 需要消费该消息。

#### Producer
```
// Producer.ts

import * as kafka from 'kafka-node'

const client = new kafka.KafkaClient({kafkaHost: 'localhost:9092'})

const producer = new kafka.HighLevelProducer(client)
producer.on('ready', function () {
  console.log('Kafka Producer is connected and ready.')
})

// For this demo we just log producer errors to the console.
producer.on('error', function (error) {
  console.error(error)
})

const sendRecord = (objData, cb) => {
  const buffer = Buffer.from(JSON.stringify(objData))

  // Create a new payload
  const record = [
    {
      topic: 'webevents.dev',
      messages: buffer,
      attributes: 1 /* Use GZip compression for the payload */
    }
  ]

  // Send record to Kafka and log result/error
  producer.send(record, cb)
}

let times = 0

setInterval(() => {
  sendRecord({
    msg: `this is message ${++times}!`
  }, (err, data) => {
    if (err) {
      console.log(`err: ${err}`)
    }
    console.log(`data: ${JSON.stringify(data)}`)
  })
}, 1000)

```

#### Consumer

```
// Consumer.ts
import * as kafka from 'kafka-node'

const client = new kafka.KafkaClient({kafkaHost: 'localhost:9092'})

const topics = [
  {
    topic: 'webevents.dev'
  }
]
const options = {
  autoCommit: true,
  fetchMaxWaitMs: 1000,
  fetchMaxBytes: 1024 * 1024
  // encoding: 'buffer'
}
// { autoCommit: false, fetchMaxWaitMs: 1000, fetchMaxBytes: 1024 * 1024 };

const consumer = new kafka.Consumer(client, topics, options)

consumer.on('message', function (message) {

  // Read string into a buffer.
  console.info(`[message]:==:>${JSON.stringify(message)}`)
  const buf = new Buffer(String(message.value), 'binary')
  const decodedMessage = JSON.parse(buf.toString())

  console.log('decodedMessage: ', decodedMessage)
})

consumer.on('error', function (err) {
  console.log('error', err)
})

process.on('SIGINT', function () {
  consumer.close(true, function () {
    process.exit()
  })
})

```